{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная 3 - Знакомство со сверточными нейронными сетями; обучение нейронных сетей\n",
    "\n",
    "В этой лабораторной вы познакомитесь что такое сверточные нейронные сети (Convolutional Neural Networks, CNN), как их применять для задачи классификации изображений и как правильно организовать процесс обучения нейронных сетей.\n",
    "\n",
    "Сыерточные нейронные сети были разработаны французским ученым Яном Лекуном (Yann LeCun) в конце 80-х годов. Они сразу показали свою высокую эффективность при обработке изображений. Это происходит из-за того, что сеть не только учится воспринимать значения пикселей, но и учится распознавать отношения между рядом стоящими пикселями.\n",
    "\n",
    "Простейшая многослойная сверточная сеть состоит из трех типов слоев: сверточных слоев, слоев пулинга и полносвязных слоев (с которыми вы познакомились в предыдущей лабораторной).\n",
    "\n",
    "То, как работает слой свертки показано на следующей анимации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![работа сверточного слоя](img/ConvLayer.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слева на этой анимации показана матрица входных данных. Для простоты, можно домуть о ней как об одноканальном черно-белом изображении, состоящем из пикселей. \n",
    "Красным квадратом показано окно свертки. В данном случае, оно имеет размер 3 x 3 пикселя. Каждое значение из этого окно домножается на определенный вес (это те самые веса, которые будут корректироваться в ходе обучения сети). Матрица весов представлена в середине рисунка. Фильтр скользит по исходному изображению с определенным шагом.\n",
    "Справа показана матрица, получающаяся в результате применения фильтра свертки.\n",
    "\n",
    "Предположим, что в данный момент времени, центральный пиксель фильтра находится в координате (1, 1). Посчитаем, какое значение фильтра будет в этом случае:\n",
    "$$(7 * 0) + (6 * (-1)) + (5 * 0)+$$\n",
    "$$(6 * (-1)) + (4 * 5) + (3 * (-1))+$$\n",
    "$$(5 * 0) + (3 * -1) + (2 * 0)=2$$\n",
    "\n",
    "У сверточного слоя таких фильтров может быть неколько. Они работают параллельно, независимо друг от друга. Количество сверточных фильтров в рамках одного слоя называется количеством каналов исходного тензора (напоминаю, что тензором называется многомерная матрица). \n",
    "\n",
    "Помимо количества фильтров, у сверточных слоев есть два дополнительных параметра. Первый из них - padding. Он показывает на сколько пикселей нужно расширить каждую сторону исходной матрицы, когда фильтр дойдет до этой стороны. На анимации сверху padding равняется одному. Второй параметр - strides. Он задает с каким шагом перемещается окно фильтра. В примере выше strides равен 1, потому что фильтр шагает вправо и вниз с шагом 1. Детальную визуализацию того, как работает padding и strides можно увидеть по следующей ссылке [Padding и Strides](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь перейдем к рассмотрению pooling слоев. Визуализация работы pooling слоя представлена на следующем рисунке:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MaxPooling](img/Max_pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У pooling слоев тоже есть окна, с которыми они предвигаются по изображению. Но вместо того, чтобы брать сумму произведений пикселей на веса, они просто берут максимальное/минимальное/среднее значение пикселей из области окна. Когда берется максимальное значение, pooling называется MaxPooling; минимальное значение - MinPooling; среднее значение - AvgPooling. Как видно из описания, в pooling-слоях нет весов, соответственно, они не являются обучаемыми слоями. \n",
    "\n",
    "На рисунке сверху представлен пример работы MaxPooling. Каждая итерация работы окна показана разным цветом. Слева представлено исходная матрица, а справа - получившийся результат. Видно, что в отличие от сверточных слоев при скольжении окна по исходной матрице не происходит наложение одного окна на другое.\n",
    "\n",
    "Теперь давайте применим полученные знания на практике и построим классификатор изображений, основанный на сверточных нейронных сетях.\n",
    "\n",
    "Для начала импортируем библиотеку PyTorch и библиотеку torchvision, в которой содержится много полезных функций для работы с изображениями. Для установки библиотеки torchvision необходимо использовать команду `pip install torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие фреймворки машинного обучения могут производить вычисления не только на CPU, но и на видеокартах - GPU. Вычисление на GPU значительно повышает скорость вычислений. По этому, если в вашем распоряжении есть видеокарта с поддержкой Nvidia CUDA, то вы можете использовать ее для работы с нейронными сетями. Для того, чтобы определить есть ли в вашем компьютере такая видеокарта или нет, в PyTorch есть функция torch.cuda.is_available( ), которая возвращает True, если GPU имеется и False в противном случае. Чтобы использовать в дальнейшем это устройство, нужн осоздать экземпляр утройства это делается с помощью torch.device( ) в который нужно передать, либо 'cuda', если вы хотите вычислять на GPU, либо 'cpu' - если на центральном процессоре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "# subprocess.run([\"ls\"])\n",
    "proc = subprocess.Popen([\"ls\"], stdout=subprocess.PIPE, shell = True )\n",
    "out, err = proc.communicate()\n",
    "print(out.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее подготовим данные для обучения нашей сети. В качестве данных мы будем использовать датасет CIFAR10. Этот датасет состоит из картинок, на каждой из которых изображен какой-то объект. Всего классов таких объектов 10 (самолет, легковая машина, птица, кошка и т.д.). \n",
    "\n",
    "Когда все обучающие примеры пройдут через сеть закончится одна эпоха обучения. Таких эпох может быть несколько. После каждой эпохи обучения принято тестировать модель, чтобы проверить то, как она реагирует на новые, еще не увиденные ею данные. По весь набор данных принято разбивать на два подмножества - одно для обучения (обучающая выборка) и одно для тестирования (тестовая выборка). Обычно, в обучающую выборку попадает 70% всех данных. \n",
    "\n",
    "Кроме того, чтобы обучение сети проходило более эффективно, принято перемешивать обучающую выборку. Это делается с той целью, чтобы данные, относящиеся к одним и тем же классам не стояли рядом друг с другом. \n",
    "\n",
    "Напоминаю, что вам уже известно, что сеть может обрабатывать несколько примеров за один раз. Это называется батчем. В данном случае размер батча равен 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Указываем, какие трансформации нужно производить с исходными данными\n",
    "# перед тем, как они попадут в сеть.\n",
    "# Чтобы задать набор трансформаций, нужн овоспользоваться классом\n",
    "# transforms.Compose( ), в который нужно передать список из классов-трансформаций.\n",
    "# В данном случае мы используем преобразование исходных данных в тензор PyTorch: transforms.ToTensor( )\n",
    "# и нормализуем изображение, чтобы каждый пиксель принимал значение из нормального распределения.\n",
    "# В данном случае, у нас изображение является трехканальным, по этому мы указываем наборы из трех значений.\n",
    "# Первый набор - среднее значение нормального распределения, а второе значение - его среднеквадратическое отклонение\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Создаем набор данных для обучения. Для этого мы будеем использовать уже имеющийся в библиотеке датасет.\n",
    "# Параметр root показывает, куда скачивать датасет;\n",
    "# train - показывает, что эти обучающая выборка;\n",
    "# download - показывает, что данных у нас нет в директории root и их необходимо скачать\n",
    "# transform - мы указываем, какие трансформации нужно произвести с каждым изображением.\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "# После того, как данные загружены, мы создаем генератор батчей.\n",
    "# В него мы передаем наши данные.\n",
    "# batch_size - размер батча\n",
    "# shuffle - нужно ли перемешивать данные\n",
    "# num_workers - сколько ядер процессора использовать для создания генератора\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "# Загружаем данные для тестирования сети\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "# Создаем генератор батчей для тестового набора данных\n",
    "testloader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog',\n",
    "          'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем архитектуру нашего классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # Конструктор нашей сети, в котором описываются все слои, которые будут в ней использоваться\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # При конструировании архитектур сетей очень важно уделять внимание размерности данных на каждом каждом этапе работы сети.\n",
    "        # Это позволяет контролировать то, что происходит на каждом слое и избегать несогласованности размерностей\n",
    "        # Задаем переменную-флаг, чтобы выводить информацию только о первом батче\n",
    "        self.first_batch = True\n",
    "        \n",
    "        # Сверточный слой для обработки двухмерных изображений описываются с помощью слоя Conv2d.\n",
    "        # В конструктор передаются следующие параметры:\n",
    "        # первый параметр - количество каналов входных данных (их 3, потому что у нас изображение цветное)\n",
    "        # второй параметр - количество каналов выходного изображения (т.е. количество фильтров свертки)\n",
    "        # третий параметр - размер скользящего окна (в этом случае оно имеет размер 5 х 5)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # Для применения MaxPooling-а к двухмерной матрице используется слой MaxPool2d\n",
    "        # В конструктор класса передаются следующие параметры:\n",
    "        # первый параметр - размер окна pooling-а (в нашем случае окно 2 х 2)\n",
    "        # второй параметр - на сколько сдвигать окно на каждой итерации\n",
    "        # (в большинстве случаев этот параметр совпадает с размерами окна)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Второй сверточный слой\n",
    "        # число входных каналов совпадает с числом выходных каналов у предыдущего сверточного слоя\n",
    "        # число выходных каналов - 16\n",
    "        # размер окна свертки - 5 x 5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Для проведения классификации изображений используется трехслойная полносвязная сеть\n",
    "        # Первый слой этой сети имеет размерность входных данных - 400.\n",
    "        # Это число берется из следующего расчета: 16 каналов после второй свертки * \n",
    "        # 5 * 5 (размер матрицы изображения, получившийся после применения сверток и pooling-а).\n",
    "        # Число нейронов на этом слое - 120.\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        # Второй полносвязный слой\n",
    "        # Размерность входных данных - 120 (совпадает с количеством нейронов на предыдущем слое)\n",
    "        # Число нейронов - 84\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # Третий полносвязный слой\n",
    "        # Размерность входных данных - 84\n",
    "        # Число нейронов - 10 (важно, чтобы число нейронов на последнем слое классификатора совпадала с числом классов)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    # Задаем последовательность взаимодействия слоев сети\n",
    "    def forward(self, x):\n",
    "        # Выводим размерность входных данных у первого батча\n",
    "        # Первая координата - размер батча\n",
    "        # Вторая координата - количество каналов\n",
    "        # Третья и четвертая координата - высота и ширина матрицы изображения\n",
    "        if self.first_batch:\n",
    "            # (8, 3, 32, 32)\n",
    "            print('Input data shape: {}'.format(x.shape))\n",
    "            \n",
    "        # Применяем первый сверточный слой.\n",
    "        # Для этого слоя используем функцию активации ReLU.\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # Выводим размерность данных после первого сверточного слоя\n",
    "        if self.first_batch:\n",
    "            # (8, 6, 28, 28)\n",
    "            print('Data shape after 1-st conv: {}'.format(x.shape))\n",
    "        \n",
    "        # Применяем слой MaxPooling\n",
    "        x = self.pool(x)\n",
    "        # Можно заметить, что размер матрицы изображения уменьшился в 2 раза.\n",
    "        # Это происходит из-за того, что для pooling-а мы использовали окно 2\n",
    "        if self.first_batch:\n",
    "            # (8, 6, 14, 14)\n",
    "            print('Data shape after 1-st pooling: {}'.format(x.shape))\n",
    "        \n",
    "        # Применяем второй сверточный слой.\n",
    "        # Для этого слоя используем функцию активации ReLU.\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if self.first_batch:\n",
    "            # (8, 16, 10, 10)\n",
    "            print('Data shape after 2-nd conv: {}'.format(x.shape))\n",
    "        \n",
    "        # Снова применяем слой MaxPooling. Напоминаю, что в слое MaxPooling-а нет обучаемых параметров.\n",
    "        # По этому можно применять один и тот же слой несколько раз.\n",
    "        x = self.pool(x)\n",
    "        if self.first_batch:\n",
    "            # (8, 16, 5, 5)\n",
    "            print('Data shape after 2-nd pooling: {}'.format(x.shape))\n",
    "        \n",
    "        # Для уменьшения размерности тензора используется функция view.\n",
    "        # Там, где мы хотим зафиксировать размерность, мы ставим -1 (всего может быть только одна -1)\n",
    "        # А дальше, указываем новые размерности. При этом нужно следить, чтобы произведение всех размерностей у исходного тензора\n",
    "        # совпадала с произведением размерностей, указанных в этой функции, за исключением -1.\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        # Видим, что после применения функции view тензор из четырехмерного превратился в двухмерный\n",
    "        # При этом размер второй координаты равен произведению координат 2, 3 и 4 в исходном тензоре\n",
    "        if self.first_batch:\n",
    "            # (8, 400)\n",
    "            print('Data shape after view applying: {}'.format(x.shape))\n",
    "        \n",
    "        # Применяем первый полносвязный слой с функцией активации ReLU   \n",
    "        x = F.relu(self.fc1(x))\n",
    "        if self.first_batch:\n",
    "            # (8, 120)\n",
    "            print('Data shape after 1-st fullyconnected: {}'.format(x.shape))\n",
    "        \n",
    "        # Применяем второй полносвязный слой с функцией активации ReLU\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if self.first_batch:\n",
    "            # (8, 84)\n",
    "            print('Data shape after 2-nd fullyconnected: {}'.format(x.shape))\n",
    "        \n",
    "        # Применяем третий полносвязный слой. Важно, чтобы у классификатора на последнем слое не стояло функций активации!\n",
    "        x = self.fc3(x)\n",
    "        if self.first_batch:\n",
    "            # (8, 10)\n",
    "            print('Data shape after 3-rd fullyconnected: {}'.format(x.shape))\n",
    "            self.first_batch = False\n",
    "        return x\n",
    "    \n",
    "# Создаем экземпляр нашей сети   \n",
    "net = Net()\n",
    "# Переносим сеть на указанное нами устройство\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим гиперпараметры обучения нашей сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5          # Количество эпох обучения\n",
    "L_RATE = 0.0001       # Скорость обучения\n",
    "MOMENTUM = 0.8        # Момент\n",
    "PRINT_EVERY = 2000    # интервал для вывода результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем функцию ошибки и оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Поскольку у нас задача классификации, где есть лейблы, представленные целыми числами\n",
    "# Мы используем функцию ошибки перекрестная энтропия\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# В качестве оптимизатора используется стохастический градиентный спуск\n",
    "# При этом, к скорости обучения мы добавляем еще один параметр - момент,\n",
    "# который показывает, на сколько быстрее мы должны спускаться по той координате, по которой функция имеет большую скорость убывания\n",
    "optimizer = optim.SGD(net.parameters(), lr=L_RATE, momentum=MOMENTUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая будет вычислять точность нашей сети на тестовых данных. Точность вычисляется по следующей формуле:\n",
    "\n",
    "$$acc=\\frac{количество\\_верных\\_ответов}{общее\\_количество\\_ответов}$$\n",
    "\n",
    "Точность изменяется в диапазоне [0,1]. Часто точность представляют в виде процентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(model):\n",
    "    correct = 0 # Счетчик для общего числа верных ответов\n",
    "    total = 0 # Счетчик для общего числа ответов\n",
    "    # Поскольку в ходе тестирования не нужно изменять веса модели, указываем, что градиенты брать не нужно\n",
    "    with torch.no_grad():\n",
    "        # Перебираем батчи.\n",
    "        # testloader - это итератор, который создал батчи за нас.\n",
    "        # он возвращает пару (входные данные, лейблы)\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # Переносим входные данные и лейблы на устройство\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Посылаем входные данные в сеть и получаем результат работы сети\n",
    "            outputs = model(images)\n",
    "            # В качестве предсказанного лейбла выбираем максимальное из 10 значений вектора, получившегося на выходе сети\n",
    "            # Функция torch.max возвращает два значения: первое - это само максимальное число, а второе - позиция, на которой оно стоит\n",
    "            # Передаем в функцию тензор и измерение, вдоль которого нужно искать максимум\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # прибавляем к счетчику общего числа ответов размер батча\n",
    "            total += labels.size(0)\n",
    "            # высчитываем количество совпадений результатов работы сети с указанными лейблами\n",
    "            # и прибовляем это количество к счетчику правильных ответов\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct/ total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цикл обучения сети. Напоминаю, что каждая итерация перебора всех обучающих примеров называется эпохой. Количество эпох ля нашего случая мы задали выше. В ходе каждой эпохи нужно выполнить несколько действий:\n",
    "* перебрать все батчи;\n",
    "* посичтать точность на тестовых данных;\n",
    "* сохранить веса сети для дальнейшего их использования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([8, 3, 32, 32])\n",
      "Data shape after 1-st conv: torch.Size([8, 6, 28, 28])\n",
      "Data shape after 1-st pooling: torch.Size([8, 6, 14, 14])\n",
      "Data shape after 2-nd conv: torch.Size([8, 16, 10, 10])\n",
      "Data shape after 2-nd pooling: torch.Size([8, 16, 5, 5])\n",
      "Data shape after view applying: torch.Size([8, 400])\n",
      "Data shape after 1-st fullyconnected: torch.Size([8, 120])\n",
      "Data shape after 2-nd fullyconnected: torch.Size([8, 84])\n",
      "Data shape after 3-rd fullyconnected: torch.Size([8, 10])\n",
      "EPOCH: 1, iter: 0 | loss: 0.0012\n",
      "EPOCH: 1, iter: 2000 | loss: 2.3021\n",
      "EPOCH: 1, iter: 4000 | loss: 2.3008\n",
      "EPOCH: 1, iter: 6000 | loss: 2.2974\n",
      "Accuracy of the network: 10.97%\n",
      "EPOCH: 2, iter: 0 | loss: 0.0011\n",
      "EPOCH: 2, iter: 2000 | loss: 2.2908\n",
      "EPOCH: 2, iter: 4000 | loss: 2.2822\n",
      "EPOCH: 2, iter: 6000 | loss: 2.2635\n",
      "Accuracy of the network: 17.27%\n",
      "EPOCH: 3, iter: 0 | loss: 0.0011\n",
      "EPOCH: 3, iter: 2000 | loss: 2.2265\n",
      "EPOCH: 3, iter: 4000 | loss: 2.1951\n",
      "EPOCH: 3, iter: 6000 | loss: 2.1453\n",
      "Accuracy of the network: 23.73%\n",
      "EPOCH: 4, iter: 0 | loss: 0.0011\n",
      "EPOCH: 4, iter: 2000 | loss: 2.0987\n",
      "EPOCH: 4, iter: 4000 | loss: 2.0454\n",
      "EPOCH: 4, iter: 6000 | loss: 2.0082\n",
      "Accuracy of the network: 27.7%\n",
      "EPOCH: 5, iter: 0 | loss: 0.0009\n",
      "EPOCH: 5, iter: 2000 | loss: 1.9602\n",
      "EPOCH: 5, iter: 4000 | loss: 1.9400\n",
      "EPOCH: 5, iter: 6000 | loss: 1.9152\n",
      "Accuracy of the network: 31.56%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Перебираем эпохи\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    # Заводим переменную, в которую будем накапливать ошибку после каждого вывода средней ошибки на экран\n",
    "    running_loss = 0.0\n",
    "    # Перебираем батчи.\n",
    "    # trainloader - это итератор, который создал батчи за нас.\n",
    "    # он возвращает пару (входные данные, лейблы)\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        # Переносим входные данные и лейблы на устройство\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Посылаем входные данные в сеть и получаем результат работы сети\n",
    "        outputs = net(inputs)\n",
    "        # Считаем функцию ошибки между результатами работы сети и указанными нами лейблами\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Считаем градиент\n",
    "        loss.backward()\n",
    "        # С помощью оптимизатора делаем шаг в направлении противоположном градиенту\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Прибавляем ошибку на данной итерации к общей ошибке\n",
    "        # Чтобы превратить тензор, содержащий одно число, в вещественное значение, используют функцию item( )\n",
    "        running_loss += loss.item()\n",
    "        # Если номер нашего батча кратен указанному нами числу...\n",
    "        if i % PRINT_EVERY == 0:\n",
    "            # выводим на экран среднюю ошибку за указанный промежуток\n",
    "            print('EPOCH: {}, iter: {} | loss: {:.4f}'.format(epoch, i, running_loss / PRINT_EVERY))\n",
    "            # и обнуляем накопитель ошибки\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # Используем написанную нами функцию для подсчета точности на тестовой выборке\n",
    "    epoch_acc = compute_acc(net)\n",
    "    print('Accuracy of the network: {:.4}%'.format(epoch_acc))\n",
    "    \n",
    "    # Сохраняем веса сети\n",
    "    torch.save(net.state_dict(), './trained_net_{}_{:.4f}.weight'.format(epoch, epoch_acc))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на все файлы с весами, которые были созданы в ходе обучения сети. Последнее вещественное число в названии файла - точность на тестовой выборке для данного набора весов. Соответственно, мы должны использовать в дальнейшей работе файл с самой высокой точностью. Для просмотра содержимого директории можно воспользоваться командой ls. Чтобы выполнять комады операционной системы, а не питона, нужно поставить перед ними восклицательный знак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"ls\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как мы сохранили веса сети, их можно использовать, не обучая сеть каждый раз. Для этого достаточно создать экземпляр сети и загрузить в него веса. Важно помнить, чтобы архитектура экземпляра сети полностью совпадала с архитектурой сети, веса которой были сохранены!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([8, 3, 32, 32])\n",
      "Data shape after 1-st conv: torch.Size([8, 6, 28, 28])\n",
      "Data shape after 1-st pooling: torch.Size([8, 6, 14, 14])\n",
      "Data shape after 2-nd conv: torch.Size([8, 16, 10, 10])\n",
      "Data shape after 2-nd pooling: torch.Size([8, 16, 5, 5])\n",
      "Data shape after view applying: torch.Size([8, 400])\n",
      "Data shape after 1-st fullyconnected: torch.Size([8, 120])\n",
      "Data shape after 2-nd fullyconnected: torch.Size([8, 84])\n",
      "Data shape after 3-rd fullyconnected: torch.Size([8, 10])\n",
      "31.562\n"
     ]
    }
   ],
   "source": [
    "# Создаем новый экземпляр сети\n",
    "net = Net()\n",
    "# Считываем веса из файла и загружаем их в сеть\n",
    "net.load_state_dict(torch.load('./trained_net_5_31.5620.weight'))\n",
    "# Перносим сеть на устройство, на котором нужно производить вычисления\n",
    "net = net.to(device)\n",
    "\n",
    "# В качестве примера выведем точность данного комплекта весов на тестовой выборке.\n",
    "# Сравним полученную точность с точностью в названии файла и убедимся, что они одинаковы.\n",
    "acc = compute_acc(net)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Упражнение\n",
    "\n",
    "Давайте применим полученные нами знания на практике и напишем собственный классификатор изображений.\n",
    "\n",
    "В качестве набора данных будем использовать датасет MNIST. Этот датасет состоит из изображений рукописных цифр от 0 до 9. Каждое изображение имеет размер 28 х 28 пикселей и имеет всего один канал - черно-белый. \n",
    "\n",
    "Датасет был представлен Яном Лекуном в 1998 году. Найти более детальную информацию можно на официальном сайте датасета [MNIST](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "Примеры изображений из датасета MNIST можно увидеть на изображении ниже:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![пример семплов из датасета MNIST](img/mnist.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задайте устройство на котором будут производится вычисления:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задайте следующие преобразования входных изображений:\n",
    "# 1) преобразование входного изображения к тензору PyTorch\n",
    "# 2) нормализация входного изображения со следующими параметрами:\n",
    "#    математическое ожидание - 0.5, среднеквадратическое отклонение - 0.2 \n",
    "# Входное изображение черно-белое, по этому у него будет всего один канал\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.2))])\n",
    "\n",
    "# Скачивание обучающего множества изображений из набора данных MNIST\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "# Создайте генератор батчей для обучающей выборки\n",
    "# Размер батча задайте равным 64\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "# Скачивание тестового множества изображений из набора данных MNIST\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "# Создаем генератор батчей для тестовой выборки\n",
    "# Размер батча задайте равным 64\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6',\n",
    "          '7', '8', '9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее зададим архитектуру нашего классификатора. Он будет состоять из следующих слоев:\n",
    "* сверточный слой, выходных каналов - 4, размер окна - 3, функция активации - ReLU;\n",
    "* слой Max Pooling, размер окна - 2, сдвиг окна - 2;\n",
    "* сверточный слой, выходных каналов - 8, размер окна - 3, функция активации - ReLU;\n",
    "* слой Max Pooling, размер окна - 2, сдвиг окна - 2;\n",
    "* сверточный слой, выходных каналов - 16, размер окна - 3, функция активации - ReLU;\n",
    "* полносвязный слой, нейронов в слое - 64, функция активации - ReLU;\n",
    "* полносвязный слой, нейронов в слое - 32, функция активации - ReLU;\n",
    "* полносвязный слой, нейронов в слое - 16, функция активации - ReLU;\n",
    "* полносвязный слой, нейронов в слое - 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # Конструктор нашей сети, в котором описываются все слои, которые будут в ней использоваться\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(1, 4, 3)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3)\n",
    "        self.conv3 = nn.Conv2d(8, 16, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 3 * 3, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 10)\n",
    "        \n",
    "    # Опишите последовательность работы слоев сети\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x)) \n",
    "        x = x.view(-1, 16 * 3 * 3)\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x)) \n",
    "        x = F.relu(self.fc3(x)) \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "# Создаем экземпляр нашей сети   \n",
    "net = Net()\n",
    "# Перенесите сеть на указанное устройство\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим гиперпараметры обучения сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10          # Количество эпох обучения\n",
    "L_RATE = 0.001         # Скорость обучения\n",
    "MOMENTUM = 0.8         # Момент\n",
    "PRINT_EVERY = 500      # интервал для вывода результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию ошибки и оптимизатор. В качестве оптимизатора используйте алгоритм стохастического градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Определим функцию ошибки\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Создадим оптимизатор\n",
    "optimizer = optim.SGD(net.parameters(), lr=L_RATE, momentum=MOMENTUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию, вычисляющую точность работы сети на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct/ total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишите цикл обучения сети. Напоминаю, что каждая эпоха должна состоять из трех этапов:\n",
    "* перебор всех обучающих батчей\n",
    "* определение качества работы сети на тестовых данных\n",
    "* сохранение весов сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, iter: 0 | loss: 0.0046\n",
      "EPOCH: 1, iter: 500 | loss: 2.3144\n",
      "Accuracy of the network: 9.82%\n",
      "EPOCH: 2, iter: 0 | loss: 0.0047\n",
      "EPOCH: 2, iter: 500 | loss: 2.3006\n",
      "Accuracy of the network: 13.54%\n",
      "EPOCH: 3, iter: 0 | loss: 0.0046\n",
      "EPOCH: 3, iter: 500 | loss: 2.2777\n",
      "Accuracy of the network: 28.4%\n",
      "EPOCH: 4, iter: 0 | loss: 0.0043\n",
      "EPOCH: 4, iter: 500 | loss: 1.9236\n",
      "Accuracy of the network: 71.63%\n",
      "EPOCH: 5, iter: 0 | loss: 0.0018\n",
      "EPOCH: 5, iter: 500 | loss: 0.7390\n",
      "Accuracy of the network: 82.13%\n",
      "EPOCH: 6, iter: 0 | loss: 0.0011\n",
      "EPOCH: 6, iter: 500 | loss: 0.5147\n",
      "Accuracy of the network: 86.02%\n",
      "EPOCH: 7, iter: 0 | loss: 0.0007\n",
      "EPOCH: 7, iter: 500 | loss: 0.4157\n",
      "Accuracy of the network: 88.63%\n",
      "EPOCH: 8, iter: 0 | loss: 0.0007\n",
      "EPOCH: 8, iter: 500 | loss: 0.3507\n",
      "Accuracy of the network: 91.21%\n",
      "EPOCH: 9, iter: 0 | loss: 0.0005\n",
      "EPOCH: 9, iter: 500 | loss: 0.2947\n",
      "Accuracy of the network: 92.58%\n",
      "EPOCH: 10, iter: 0 | loss: 0.0004\n",
      "EPOCH: 10, iter: 500 | loss: 0.2484\n",
      "Accuracy of the network: 92.58%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Перебираем эпохи\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % PRINT_EVERY == 0:\n",
    "            print('EPOCH: {}, iter: {} | loss: {:.4f}'.format(epoch, i, running_loss / PRINT_EVERY))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    epoch_acc = compute_acc(net)\n",
    "    print('Accuracy of the network: {:.4}%'.format(epoch_acc))\n",
    "    \n",
    "    torch.save(net.state_dict(), './trained_net_{}_{:.4f}.weight'.format(epoch, epoch_acc))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте новый экземпляр класса сети, и загрузите в него обученные веса с самой высокой точностью на тестовых данных. Посчитайте точность для новой сети. Сравните ее с максимальной точностью, полученной в ходе обучения сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.58\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load('./trained_net_10_92.5800.weight'))\n",
    "net = net.to(device)\n",
    "acc = compute_acc(net)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
