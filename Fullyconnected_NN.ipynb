{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Написание нейронных сетей на PyTorch\n",
    "\n",
    "В этом ноутбуке мы разберем как писать нейронные сети на фреймворке PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полносвязная нейронная сеть\n",
    "\n",
    "Полносвязная нейронная сеть - простейший вид нейронных сетей. Но не смотря на это, такие сети применяются очень часто. В полносвязной нейронной сети каждый нейрон одного слоя связан с каждым нейроном последующего слоя. Пример можно увидеть на изображении:\n",
    "\n",
    "![полносвязная нейронная сеть](img/FC_NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом изображении показана нейронная сеть, состоящая из одного входного слоя (зеленый), двух скрытых слоев (синий) и одного выходного слоя (красный).\n",
    "\n",
    "Давайте реализуем такую архитектуру на фреймворке PyTorch. PyTorch - это внешняя библиотека по этому для ее установки нужно использовать команду `pip install pytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnected(\n",
      "  (hidden1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (hidden2): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (out): Linear(in_features=3, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn # Импортируем модуль nn, в котором содержется функционал для написания нейронных сетей\n",
    "import torch.nn.functional as F # Импортируем модуль functional, в котором содержатся полезные функции для работы с сетками\n",
    "\n",
    "# Каждая нейронная сеть представляет из себя класс, который наследуется от класса nn.Module\n",
    "class FullyConnected(nn.Module):\n",
    "    # В любой нейронной сети должен быть конструктор\n",
    "    def __init__(self):\n",
    "        # В конструкоре обязательно нужно вызвать конструктор базового класса nn.Module\n",
    "        super(FullyConnected, self).__init__()\n",
    "        \n",
    "        # Далее идет описание слоев сети\n",
    "        # Полносвязный слой описывается с помощью класса nn.Linear.\n",
    "        # Первым параметром конструктора является размерность входящих в него векторов \n",
    "        # (т.е. cколько нейронов стояло на предыдущем слое)\n",
    "        # Вторым параметром конструктора является количество нейронов данного слоя \n",
    "        self.hidden1 = nn.Linear(4, 4) # Описываем первый скрытый слой (на вход ему поступает вектор размерности 4; в слое 4 нейрона)\n",
    "        self.hidden2 = nn.Linear(4, 3) # Описываем второй скрытый слой (на вход ему поступает вектор размерности 4; в слое 3 нейрона)\n",
    "        self.out = nn.Linear(3, 2) # Описываем выходной слой (на вход ему поступает вектор размерности 3; в слое 2 нейрона)\n",
    "    \n",
    "    # В функции forward описывается логика работы сети, т.е. как информация от одного слоя пердается к другому\n",
    "    # x - входные данные для сети. x - матрица, размер которой - (размер_батча, размер_входных_данных)\n",
    "    def forward(self, x):\n",
    "        # Посылаем входные данные в первый скрытый слой.\n",
    "        # После слоя добавляем функцию активации ReLU.\n",
    "        # На выходе получается матрица размером (размер_батча, 4)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        # Посылаем входные данные в второй скрытый слой.\n",
    "        # После слоя добавляем функцию активации ReLU.\n",
    "        # На выходе получается матрица размером (размер_батча, 3)\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        # Посылаем входные данные в выходной слой.\n",
    "        # После слоя добавляем функцию активации Sigmoid.\n",
    "        # На выходе получается матрица размером (размер_батча, 2)\n",
    "        x = F.sigmoid(self.out(x))\n",
    "        return x\n",
    "\n",
    "# Создаем экземпляр класса нейронной сети\n",
    "fc_nn = FullyConnected()\n",
    "# Выводим на экран архитектуру нашей сети\n",
    "print(fc_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим нашу сеть. Для начала создадим входную и выходную переменные - наши обучающие данные "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch # импортируем pytorch\n",
    "# импортируем класс-обертку Variable, который позволяет пропускать градиенты через переменные\n",
    "from torch.autograd import Variable \n",
    "\n",
    "# Для начала создадим тензор размерности 64 х 4 - входные данные, заполненный случайными вещественными числами\n",
    "# Оборачиваем созданный тензор в обертку Variable \n",
    "# Указываем, что вычисления будут проходить на CPU\n",
    "x = Variable(torch.randn((64, 4))).to('cpu')\n",
    "# Создадим тензор размерности 64 - выходные лейблы для задачи классификации, заполненный случайными целыми числами 0 или 1\n",
    "labels = Variable(torch.randint(0, 1, (64,))).to('cpu')\n",
    "\n",
    "# Выведем размерности тензоров\n",
    "print(x.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим гиперпараметры нашей сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1000 # Количество эпох обучения\n",
    "LEARNING_RATE = 0.1 # Коэффициент скорости обучения\n",
    "PRINT_EVERY = 50 # Через сколько эпох нужно выводить результаты на экран"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Укажем функцию ошибки и алгоритм обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim # Импортируем модуль optim, в котором содержатся классы-оптимизаторы сетей\n",
    "\n",
    "# В качестве функции ошибки выберем кросс-энтропию, которая используется в задачах классификации\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# В качестве алгоритма обучения будем использовать стохастический градиентный спуск\n",
    "# Первым параметром для конструктора класса являются веса сети, которые нужно оптимизировать\n",
    "# Кроме того, передаем в него скорость обучения\n",
    "optimizer = optim.SGD(fc_nn.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее опишем цикл обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Loss: 0.313386\n",
      "Epoch: 100 | Loss: 0.313385\n",
      "Epoch: 150 | Loss: 0.313383\n",
      "Epoch: 200 | Loss: 0.313382\n",
      "Epoch: 250 | Loss: 0.313381\n",
      "Epoch: 300 | Loss: 0.313379\n",
      "Epoch: 350 | Loss: 0.313378\n",
      "Epoch: 400 | Loss: 0.313377\n",
      "Epoch: 450 | Loss: 0.313376\n",
      "Epoch: 500 | Loss: 0.313375\n",
      "Epoch: 550 | Loss: 0.313374\n",
      "Epoch: 600 | Loss: 0.313373\n",
      "Epoch: 650 | Loss: 0.313371\n",
      "Epoch: 700 | Loss: 0.313370\n",
      "Epoch: 750 | Loss: 0.313369\n",
      "Epoch: 800 | Loss: 0.313368\n",
      "Epoch: 850 | Loss: 0.313367\n",
      "Epoch: 900 | Loss: 0.313366\n",
      "Epoch: 950 | Loss: 0.313365\n",
      "Epoch: 1000 | Loss: 0.313364\n"
     ]
    }
   ],
   "source": [
    "# Цикл, в котором будем перебирать эпохи обучения\n",
    "for n_epoch in range(1, N_EPOCHS+1):\n",
    "    # В начале каждой итерации обнуляем градиенты \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Посылаем входные данные в сеть и получаем выходные данные, полученные в ходе работы сети\n",
    "    output = fc_nn(x)\n",
    "    \n",
    "    # Для вычисления ошибки используем выбранный нами критерий - кросс-энтропию\n",
    "    # На вход передаем выходы сети и метки классов - лейблы, чтобы он сравнил то,\n",
    "    # что сгенерировала сеть с реальными значениями\n",
    "    loss = criterion(output, labels)\n",
    "    \n",
    "    # Вычисляем градиенты по каждому из весов сети\n",
    "    loss.backward()\n",
    "    \n",
    "    # Делаем шаг алгоритма обучения\n",
    "    optimizer.step()\n",
    "    \n",
    "    if n_epoch % PRINT_EVERY == 0:\n",
    "        print('Epoch: {} | Loss: {:.6f}'.format(n_epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Упражнения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение 1\n",
    "\n",
    "Как вы уже поняли из примера, данные в сеть поступают по батчам. Каждый батч состоит из заранее заданного числа обучающих примеров (семплов). При этом все семплы должны иметь одну и ту же размерность. При обработке текста возникает проблема: длина текстов может различаться. Однако, в нашем случае тексты статей из Википедии довольно длинные и каждую статью можно разбить на несколько семплов.\n",
    "\n",
    "В своей работе мы будем использовать особый вид нейронных сетей - языковые модели (Language Model, LM). Суть языковых моделей заключается в том, что они угадываю слово по контексту. В качестве контекста может использоваться несколько слов, стоящих перед угадываемым словом. То есть в нашем случае входными данными для сети будет контекст, а выходными - угадываемое слово.\n",
    "\n",
    "На вход сети нельзя просто подать слова, нужно подавать индексы слов. По этому давайте воспользуемся нашим словарем с собственной информацией и создадим новый словарь, отображающий слова в индексы.\n",
    "\n",
    "Для начала создадим функцию, которая будет принимать на вход словарь с собственной информацией и возвращать список пар (слово, собственная информация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def dict2pairs(word2selfinformation: dict) -> list:\n",
    "\n",
    "    result = []\n",
    "    for j in word2selfinformation:\n",
    "        result.append(j, word2selfinformation[j]))\n",
    "    return result\n",
    "\n",
    "# Прочитайте созданный вами в предыдущей лабораторной файл pickle, в котором хранится словарь слово - собственная информация\n",
    "with open('word2information.pkl', 'rb') as pkl_file:\n",
    "    word2information = pickle.load(pkl_file)\n",
    "    \n",
    "word2information_list = dict2pairs(word2information)\n",
    "print (word2information_list[0])\n",
    "print (word2information_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя собственную информацию, мы можем легко отсортировать слова по частоте их встречаемости. Давайте сделаем это (Подсказка: используйте метод .sort( ), который есть у всех списков):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция, которая принимает на вход список пар (слово, собственная информация)\n",
    "# и возвращает слова в порядке уменьшения частоты их встречаемости \n",
    "def sort_words(pairs_list: list) -> list:\n",
    "    return sorted(pairs_list,key=lambda word:word[1])\n",
    "\n",
    "sorted_words = sort_words(word2information_list)\n",
    "print(sorted_words[0])\n",
    "print(sorted_words[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда у нас есть список слов, упорядоченных в порядке уменьшения частоты их встречаемости, мы можем сопоставить каждому слову его индекс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Функция, на вход которой поступает отсортированный список слов, а на выходе - словарь \"слово-индекс\". \n",
    "# Индексация должна начинаться с 1.\n",
    "def word_index(sorted_words: list) -> dict:\n",
    "    result = {}\n",
    "    for j in sorted_words:\n",
    "        result[j[0]] = j[1]\n",
    "    return result\n",
    "\n",
    "word2index = word_index(sorted_words)\n",
    "\n",
    "print(word2index['.'])\n",
    "\n",
    "# Создадим словарь, для сохранения данных в формате pickle\n",
    "data_dict = {\n",
    "    'word2index': word2index,\n",
    "    'index2word': sorted_words\n",
    "}\n",
    "\n",
    "# Сохраним словарь в виде pickle-файла\n",
    "with open('dictionary.pkl', 'wb') as pkl_out:\n",
    "    pickle.dump(data_dict, pkl_out, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте напишем пару полезных функций, которая понадобятся нам в дальнейшем. Ин ачнем мы с функции, которая превращает текст в набор индексов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# На вход функции подается словарь \"слово-индекс\" и токенизированный текст (список токенов),\n",
    "# который мы хотим преобразовать в набор индексов. А на выходе получается список индексов.\n",
    "# Предусмотрите возможность, что слова может не оказаться в словаре. Для этого воспользуйтесь исключениями (try - except)\n",
    "def tokens2indexes(tokens: list, word2index: dict) -> list:\n",
    "    result = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            result.append(word2index[word])\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "tokens = ['i', 'might', 'consider', 'asking', 'for', 'the', 'card', 'to', 'be', 'refunded', '.']\n",
    "\n",
    "# Прочитайте только что созданный pickle-файл со словарем и возьмите из него словарь word2index\n",
    "with open('dictionary.pkl', 'rb') as pkl_in:\n",
    "    word2index = pickle.load(pkl_in)['word2index']\n",
    "\n",
    "indexes = tokens2indexes(tokens, word2index)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующая функция будет нужна нам для вычисления информации, которую содержит текст. Собственная информация текста равна сумме собственных информаций каждого из токенов текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# На вход функции подается словарь \"слово-информация\" и токенизированный текст (список токенов),\n",
    "# собственную информацию которого мы хотим найти. А на выходе получается одно единственное вещественное число -\n",
    "# \n",
    "# Предусмотрите возможность, что слова может не оказаться в словаре. Для этого воспользуйтесь исключениями (try - except)\n",
    "def text2information(tokens: list, word2information: dict) -> float:\n",
    "    result = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            result+=word2index[word]\n",
    "        except Exception:\n",
    "    return result\n",
    "\n",
    "tokens = ['i', 'might', 'consider', 'asking', 'for', 'the', 'card', 'to', 'be', 'refunded', '.']\n",
    "\n",
    "# Прочитайте созданный вами в предыдущей лабораторной файл pickle, в котором хранится словарь слово - собственная информация\n",
    "with open('word2information.pkl', 'rb') as pkl_in:\n",
    "    word2index = pickle.load(pkl_in)\n",
    "\n",
    "self_information = text2information(tokens, word2information)\n",
    "\n",
    "print(self_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение 2\n",
    "\n",
    "Теперь давайте применим на практике те знания, которые мы получили в результате разбора примера программирования глубокой нейронной сети на фреймворке PyTorch. \n",
    "\n",
    "Вам предлагается реализовать автоенкодер - сеть, которая воссоздает на выходе свои исходные данные. Пример автоенкодера приведен на рисунке:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![полносвязная нейронная сеть](img/autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый автоенкодер состоит из двух частей - енкодера, сжимающего входные данные, и декодера, восстанавливающего исходные данные. В нашем случае енкодер состоит из трех скрытых слоев, а декодер - из двух скрытых и одного выходного слоя.\n",
    "\n",
    "Для начала давайте напишем класс, описывающий автоенкодер, приведенный на изображении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.hidden1 = nn.Linear(4,4)\n",
    "        self.hidden2 = nn.Linear(4,3)\n",
    "        self.hidden3 = nn.Linear(3,2)\n",
    "        self.hidden4 = nn.Linear(2,3)\n",
    "        self.hidden5 = nn.Linear(3,4)\n",
    "        self.out = nn.Linear(4,4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = F.relu(self.hidden4(x))\n",
    "        x = F.relu(self.hidden5(x))\n",
    "        x = F.relu(self.out(x)\n",
    "        return x\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим входные данные для автоенкодера. Поскольку автоенкодер воссоздает исходные данные, роль выходных данных будут играть те же входные данные. В нашем случае сделаем размер батча 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable \n",
    "\n",
    "x = Variable(torch.randn((256, 4))).to('gpu')\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задайте гиперпараметры сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1000# Количество эпох обучения\n",
    "LEARNING_RATE = 0.00001 # Коэффициент скорости обучения\n",
    "PRINT_EVERY = 50 # Через сколько эпох нужно выводить результаты на экран"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задайте функцию ошибки и алгоритм обучения. В примере у нас была задача классификации. Теперь мы пытаемся воссоздавать исходные данные, при этом значения входных и выходных данных у нас непрерывные. Подумайте, какуюю функцию ошибки нужно применить в этом случае. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parametrs(),lr=LEARNING_RATE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте цикл обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_epoch in range(1, N_EPOCHS+1):\n",
    "    optimizer.zero_grad()\n",
    "    output = autoencoder(x)\n",
    "    loss = criterion(output, x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if n_epoch % PRINT_EVERY == 0:\n",
    "        print('Epoch: {} | Loss: {:.6f}'.format(n_epoch, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}